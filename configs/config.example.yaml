# MonsterInc Example Configuration File
# This file shows all available configuration options with their default values and explanations.
# You can copy this file to config.yaml (or config.json) and modify it to your needs.

# Global application mode. Can be "onetime" (run once) or "automated" (run periodically/continuously based on other settings).
# Default: "onetime"
mode: "onetime"

# Resource Limiter configuration: Settings for monitoring system resources and auto-shutdown.
resource_limiter_config:
  # Maximum memory usage for the application in MB before triggering garbage collection.
  # Default: 1024 (1GB)
  max_memory_mb: 1024
  # Maximum number of goroutines allowed.
  # Default: 10000
  max_goroutines: 10000
  # How often to check resource usage (in seconds).
  # Default: 30
  check_interval_secs: 30
  # Application memory threshold (percentage) before triggering warning and GC.
  # Value between 0.1 and 1.0 (0.8 = 80%)
  # Default: 0.8
  memory_threshold: 0.8
  # Goroutine count threshold (percentage) before triggering warning.
  # Value between 0.1 and 1.0 (0.7 = 70%)
  # Default: 0.7
  goroutine_warning: 0.7
  # System memory threshold (percentage) before triggering auto-shutdown.
  # When system memory usage exceeds this threshold, the application will gracefully shutdown.
  # Value between 0.1 and 1.0 (0.5 = 50% of total system memory)
  # Default: 0.5
  system_mem_threshold: 0.5
  # CPU usage threshold (percentage) before triggering auto-shutdown.
  # When CPU usage exceeds this threshold, the application will gracefully shutdown.
  # Value between 0.1 and 1.0 (0.5 = 50% of CPU usage)
  # Default: 0.5
  cpu_threshold: 0.5
  # Whether to enable automatic shutdown when resource thresholds are exceeded.
  # Default: true
  enable_auto_shutdown: true

# HTTPX Runner configuration: Settings for the httpx tool.
httpx_runner_config:
  # HTTP method to use for requests.
  # Default: "GET"
  method: "GET"
  # Specific URIs to request on each host (appended to the host). Useful for specific file checks.
  # Default: []
  request_uris:
    # - /robots.txt
    # - /.git/config
  # Number of concurrent threads for httpx.
  # Default: 25
  threads: 25
  # Maximum requests per second (0 for no limit).
  # Default: 0
  rate_limit: 0
  # Timeout in seconds for each request.
  # Default: 10
  timeout_secs: 10
  # Number of retries for failed requests.
  # Default: 1
  retries: 1
  # Whether to follow HTTP redirects.
  # Default: true
  follow_redirects: true
  # Maximum number of redirects to follow.
  # Default: 10
  max_redirects: 10
  # Custom HTTP headers to include in requests (key: value).
  # Default: {}
  custom_headers:
    # User-Agent: "MyCustomScanner/1.0"
    # X-Custom-Header: "SomeValue"
  # Enable verbose output for httpx (more detailed logging).
  # Default: false
  verbose: false
  # Enable technology detection (Wappalyzer).
  # Default: true
  tech_detect: true
  # Extract page titles.
  # Default: true
  extract_title: true
  # Extract HTTP status codes.
  # Default: true
  extract_status_code: true
  # Extract final location after redirects.
  # Default: true
  extract_location: true
  # Extract content length.
  # Default: true
  extract_content_length: true
  # Extract server header.
  # Default: true
  extract_server_header: true
  # Extract content type.
  # Default: true
  extract_content_type: true
  # Extract IP addresses of hosts.
  # Default: true
  extract_ips: true
  # Extract ASN (Autonomous System Number) information.
  # Default: true
  extract_asn: true
  # Extract HTTP response body (can be large, use with caution).
  # Default: false
  extract_body: false
  # Extract HTTP response headers.
  # Default: true
  extract_headers: true


# Crawler configuration: Settings for web crawling.
crawler_config:
  # Initial seed URLs to start crawling from. If empty, httpx results might be used as seeds.
  # Default: []
  seed_urls:
    # - https://example.com/startpage
  # User-Agent string for the crawler.
  # Default: "MonsterIncCrawler/1.0"
  user_agent: "MonsterIncCrawler/1.0"
  # Timeout in seconds for each crawler request.
  # Default: 20
  request_timeout_secs: 10
  # Maximum number of concurrent crawler requests.
  # * Reduce for more results
  # Default: 10
  max_concurrent_requests: 10
  # Maximum depth to crawl (0 for unlimited, 1 for seed pages only).
  # Default: 5
  max_depth: 5
  # Whether to respect robots.txt directives.
  # Default: true
  respect_robots_txt: false

  # Whether to skip TLS certificate verification for HTTPS requests.
  # When enabled, the crawler will accept any certificate, including self-signed or expired ones.
  # Useful for testing internal sites or sites with certificate issues.
  # Default: true
  insecure_skip_tls_verify: true


  # URL normalization: Remove duplicate URLs with only fragment or tracking parameter differences.
  # NOTE: URL normalization processing now happens at Scanner level during URL preprocessing (Step 0)
  # before URLs are passed to the crawler. This improves efficiency and deduplication.
  url_normalization:
    # Strip fragments from URLs to avoid duplicates (e.g., #section).
    # Default: true
    strip_fragments: true
    # Strip common tracking parameters (utm_*, fbclid, gclid, etc.).
    # Default: true
    strip_tracking_params: true
    # Additional custom parameters to strip from URLs.
    # Default: ["utm_source", "utm_medium", "utm_campaign", "fbclid", "gclid"]
    custom_strip_params:
      - "utm_source"
      - "utm_medium"
      - "utm_campaign"
      - "utm_content"
      - "utm_term"
      - "fbclid"
      - "gclid"
      - "ref"
      - "referrer"

  # Retry configuration: Settings for handling rate limits and failed requests.
  retry_config:
    # Maximum number of retry attempts for 429 (Too Many Requests) errors.
    # Set to 0 to disable retries.
    # Default: 3
    max_retries: 3
    # Base delay in seconds for exponential backoff.
    # The actual delay will be: base_delay * 2^attempt
    # Default: 10
    base_delay_secs: 10
    # Maximum delay in seconds for exponential backoff.
    # Delays will be capped at this value.
    # Default: 60
    max_delay_secs: 60
    # Enable jitter to randomize delays slightly to prevent thundering herd.
    # Default: true
    enable_jitter: true
    # HTTP status codes that should trigger retries.
    # Default: [429] (Too Many Requests)
    retry_status_codes:
      - 429
      # - 502  # Bad Gateway (uncomment if needed)
      # - 503  # Service Unavailable (uncomment if needed)
      # - 504  # Gateway Timeout (uncomment if needed)

    # Domain-level rate limiting: Blacklist domains with excessive rate limiting.
    domain_level_rate_limit:
      # Enable domain-level rate limiting and blacklisting.
      # Default: true
      enabled: true
      # Maximum number of 429 errors per domain before blacklisting.
      # Default: 10
      max_rate_limit_errors: 10
      # Duration to blacklist domain after hitting max errors (in minutes).
      # Default: 30
      blacklist_duration_mins: 30
      # Clear blacklist after this many hours (cleanup old entries).
      # Default: 6
      blacklist_clear_after_hours: 6

  # Crawler scope configuration: Fine-grained control over what the crawler visits.
  scope:
    # List of hostnames the crawler is explicitly allowed from visiting.
    # Default: []
    allowed_hostnames:
      - "heytapimage.com"
    # List of subdomains the crawler is explicitly allowed from visiting.
    # Default: []
    allowed_subdomains:
      - "hd.oppo.com"
    # List of hostnames the crawler is explicitly disallowed from visiting.
    # Default: []
    disallowed_hostnames:
      - "oppo.com"
      # - "internal.example.com"
    # List of subdomains the crawler is explicitly disallowed from visiting.
    # Default: []
    disallowed_subdomains:
      # - "admin" # disallows admin.example.com
    # List of file extensions (with dots) that the crawler should not visit.
    # This uses fast string comparison instead of regex for better performance.
    # Default: [".js", ".txt", ".css", ".xml"]
    disallowed_file_extensions:
      # - ".js"
      # - ".txt"
      - ".css" 
      # - ".xml"
      # - ".pdf"
      # - ".zip"
      # - ".json"
  # Maximum content length in MB to download for a page.
  # Default: 2
  max_content_length_mb: 2
  # Automatically add seed URL hostnames to allowed hostnames list.
  # When enabled, hostnames extracted from seed URLs will be automatically
  # allowed (with highest priority after disallowed rules).
  # This ensures the crawler can always visit seed domains.
  # Default: true
  auto_add_seed_hostnames: true

  # Auto-calibrate configuration: Settings for detecting and skipping similar URL patterns.
  # NOTE: Auto-calibrate processing now happens at Scanner level during URL preprocessing (Step 0)
  # before URLs are passed to the crawler. This improves efficiency and deduplication.
  auto_calibrate:
    # Whether auto-calibrate feature is enabled.
    # When enabled, URLs will be preprocessed to detect similar patterns and skip duplicates
    # during the initial preprocessing step before crawling begins.
    # Default: true
    enabled: true
    # Maximum number of similar URLs to allow per pattern before skipping.
    # For example, if set to 1, only the first URL of each pattern will be processed.
    # Default: 1
    max_similar_urls: 1
    # Parameters to ignore when detecting similar URL patterns.
    # URLs that differ only in these parameters will be considered the same pattern.
    # Default: ["tid", "fid", "page", "id", "p", "offset", "limit"]
    ignore_parameters:
      - "tid"    # Thread ID
      - "fid"    # Forum ID  
      - "page"   # Page number
      - "id"     # Generic ID
      - "p"      # Page shorthand
      - "offset" # Pagination offset
      - "limit"  # Pagination limit
    # Automatically detect and ignore locale codes in path segments
    # When enabled, path segments that look like locale codes (e.g., /en/, /fr/, /de/)
    # will be replaced with wildcards for pattern matching during preprocessing
    # Default: true
    auto_detect_locales: true
    # Custom locale codes to recognize (in addition to built-in ISO codes)
    # Default: []
    custom_locale_codes: []
    # Enable logging when URLs are skipped due to pattern similarity during preprocessing.
    # Default: true
    enable_skip_logging: true

  # Headless browser configuration: Settings for using headless browser for dynamic content crawling.
  # WARNING: On Windows, antivirus software may block the browser automation tools.
  # If you encounter errors with "leakless.exe" or "virus/potentially unwanted software",
  # the crawler will automatically fall back to traditional crawling.
  headless_browser:
    # Whether headless browser crawling is enabled.
    # When enabled, the crawler will use a headless browser to render JavaScript-heavy pages.
    # Default: false (disabled by default to avoid antivirus conflicts)
    # ! Can be slow
    enabled: false
    # Path to Chrome/Chromium executable. If empty, the system default will be used.
    # Default: ""
    chrome_path: ""
    # User data directory for the browser. If empty, a temporary directory will be used.
    # Default: ""
    user_data_dir: ""
    # Browser window width for rendering.
    # Default: 1920
    window_width: 1920
    # Browser window height for rendering.
    # Default: 1080
    window_height: 1080
    # Page load timeout in seconds.
    # Default: 30
    page_load_timeout_secs: 60
    # Additional wait time in milliseconds after page load.
    # Default: 1000
    wait_after_load_ms: 5000
    # Whether to disable image loading for faster rendering.
    # Default: true
    disable_images: true
    # Whether to disable CSS loading.
    # Default: false
    disable_css: true
    # Whether to disable JavaScript execution.
    # Default: false
    disable_javascript: false
    # Whether to ignore HTTPS certificate errors.
    # Default: true
    ignore_https_errors: true
    # Number of browser instances in the pool.
    # * Increase for reducing errors when running multiple crawling goroutines
    # Default: 3
    pool_size: 3
    # Additional browser arguments.
    # Default: ["--no-sandbox", "--disable-dev-shm-usage", "--disable-gpu"]
    browser_args:
      - "--no-sandbox"
      - "--disable-dev-shm-usage"
      - "--disable-gpu"

# Reporter configuration: Settings for generating HTML reports.
reporter_config:
  # Directory where HTML reports will be saved.
  # Default: "reports"
  output_dir: "reports/scan"
  # Number of items to display per page in paginated sections of the report.
  # Default: 25
  items_per_page: 25
  # Whether to embed assets (CSS, JS) directly into the HTML report for a single, portable file.
  # Default: true
  embed_assets: true
  # Path to a custom HTML template for the report. If empty, a default template is used.
  # Default: ""
  template_path: "" # e.g., "/path/to/custom_report_template.html"
  # Whether to generate a report even if no findings/results are available.
  # Default: false
  generate_empty_report: false
  # Title for the generated HTML report.
  # Default: "MonsterInc Scan Report"
  report_title: "MonsterInc Scan Report"
  # Enable DataTables for sortable/searchable tables in the report.
  # Default: true
  enable_data_tables: true
  # MaxProbeResultsPerReportFile defines the maximum number of probe results to include in a single HTML report file.
  # If the total number of probe results exceeds this value, the report will be split into multiple files.
  # A value of 0 means no limit (all results in one file). Default: 1000
  max_probe_results_per_report_file: 1000
  # Note: The exact report filename will be determined by the application logic (e.g., incorporating timestamps or target names).

# Storage configuration: Settings for how scan data is stored (e.g., Parquet files).
storage_config:
  # Base directory path where Parquet files (or other data) will be stored.
  # Default: "data"
  parquet_base_path: "database"
  # Compression codec to use for Parquet files (e.g., "snappy", "gzip", "zstd", "none").
  # Default: "zstd"
  compression_codec: "zstd"

# Notification configuration: Settings for sending notifications (e.g., via Discord).
notification_config:
  # Discord webhook URL for the Scan Service (onetime & automated scans).
  # Default: ""
  scan_service_discord_webhook_url: "" # e.g., "https://discord.com/api/webhooks/your/scan_service_webhook"

  # Discord webhook URL for the File Monitoring Service.
  # Default: ""
  monitor_service_discord_webhook_url: "" # e.g., "https://discord.com/api/webhooks/your/monitor_service_webhook"

  # List of Discord Role IDs to mention in notifications.
  # Default: []
  mention_role_ids:
    # - "123456789012345678" # Example Role ID
  # Send a notification when a scan completes successfully.
  # Default: false
  notify_on_success: true
  # Send a notification when a scan fails.
  # Default: true
  notify_on_failure: true
  # Send a notification when a scan starts.
  # Default: false
  notify_on_scan_start: true
  # Send a notification on critical errors during the application run.
  # Default: true
  notify_on_critical_error: true

  # Automatically delete single diff report files after a Discord notification has been successfully sent.
  # This only deletes individual diff reports (diff_*.html), keeping aggregated reports intact.
  # Default: true
  auto_delete_partial_diff_reports: true

# Logging configuration: Settings for application logging.
log_config:
  # Logging level. Determines the minimum severity of messages to be logged.
  # Valid levels: "debug", "info", "warn", "error", "fatal", "panic".
  # "debug": Detailed information, typically of interest only when diagnosing problems.
  # "info": Confirmation that things are working as expected.
  # "warn": An indication that something unexpected happened, or indicative of some problem in the near future (e.g., "disk space low"). The software is still working as expected.
  # "error": Due to a more serious problem, the software has not been able to perform some function.
  # "fatal": A severe error that will prevent the application from continuing. After logging, the application will exit.
  # "panic": Similar to fatal, but also initiates a panic.
  # Default: "info"
  log_level: "debug"

  # Log output format.
  # Valid formats: "console", "json", "text".
  # "console": Human-readable, colored output for interactive terminals.
  # "json":    Machine-readable JSON objects, one per line.
  # "text":    Plain text output, similar to console but without color codes. Suitable for simple file logging or environments where color is not supported.
  # Default: "console"
  log_format: "console"

  # Path to a log file. If empty, logs are written to standard error (stderr).
  # Example: "/var/log/monsterinc.log" or "monsterinc.log" (for current directory).
  # Default: ""
  log_file: "logs/monsterinc.log"

  # Log rotation settings (applicable when log_file is specified).
  # These settings are handled by an internal log rotation library (lumberjack).

  # Maximum size in megabytes of a log file before it gets rotated.
  # Default: 100
  max_log_size_mb: 100

  # Maximum number of old log files to retain.
  # Default: 3
  max_log_backups: 3

  # Log organization settings (NEW FEATURE)
  # Whether to organize logs into subdirectories based on scan/monitor sessions.
  # When enabled, logs will be structured as:
  # - logs/scans/{scanID}/monsterinc.log (for scan sessions)
  # - logs/monitors/{cycleID}/monsterinc.log (for monitor cycles)
  # This prevents file locking issues when multiple processes write to logs simultaneously.
  # Default: true
  use_subdirs: true



# DiffConfig: Configuration for comparing current scan results with previous ones.
diff_config:
  # Number of days to look back for previous scan data to compare against.
  # Default: 7
  previous_scan_lookback_days: 7

# DiffReporterConfig: Configuration for generating content diff reports.
diff_reporter_config:
  # Maximum file size in MB for generating a detailed line-by-line diff.
  # Files larger than this will still be reported as changed, but without a detailed diff.
  # Default: 5
  max_diff_file_size_mb: 5


# MonitorConfig: Configuration for monitoring JS/HTML files for changes.
monitor_config:
  # Whether the monitoring feature is enabled.
  # Default: false
  enabled: true
  # Interval in seconds for how often to check monitored files.
  # Default: 3600 (1 hour)
  check_interval_seconds: 180
  # Maximum number of concurrent checks for monitored files.
  # Default: 5
  max_concurrent_checks: 5
  # Whether to store the full content of a file when a change is detected.
  # If false, only hash/metadata might be stored.
  # Default: false
  store_full_content_on_change: true
  # Timeout in seconds for HTTP requests made by the monitor.
  # Default: 30
  http_timeout_seconds: 30
  # Initial list of specific URLs to monitor. Full URLs are expected.
  # Default: []
  initial_monitor_urls:
    # - https://example.com/main.js
    # - https://example.com/app.js
    # - https://anotherexample.com/index.html
  # JavaScript file extensions to monitor (fast path comparison)
  # Default: [".js", ".jsx", ".ts", ".tsx"]
  js_file_extensions:
    - ".js"
    - ".jsx"
    - ".ts"
    - ".tsx"
  # HTML file extensions to monitor (fast path comparison)
  # Default: [".html", ".htm"]
  html_file_extensions:
    - ".html"
    - ".htm"

  # Aggregation settings for monitor notifications (file changes and errors)


  # Maximum number of diff results to include in a single HTML diff report file.
  # If the total number of diff results exceeds this value, the report will be split into multiple files.
  # A value of 0 means no limit (all results in one file). Default: 500
  max_diff_results_per_report_file: 500


# Scheduler configuration: Settings for automated periodic scans (when mode is "automated").
scheduler_config:
  # Scan cycle interval in minutes. The time between the end of one scan and the start of the next.
  # Default: 10080 (equivalent to 7 days)
  cycle_minutes: 3
  # Number of retry attempts if a scheduled scan fails.
  # Default: 2
  retry_attempts: 1
  # Path to the SQLite database file used for storing scan history.
  # Default: "database/scheduler/scheduler_history.db"
  sqlite_db_path: "database/scheduler/scheduler_history.db"

# Scan Batch configuration: Settings for processing large scan target files in batches.
scan_batch_config:
  # Maximum number of targets to process in each scan batch.
  # When input file has more than threshold_size targets, it will be split into batches of this size.
  # Default: 200 (larger than monitor due to typically faster processing)
  batch_size: 200
  # Maximum number of concurrent scan batches to process at the same time.
  # Set to 0 to auto-calculate based on crawler threads (50% of threads, min 1, max 8).
  # Override with specific value if needed.
  # Default: 0 (auto-calculated)
  max_concurrent_batch: 0
  # Timeout for each scan batch processing in minutes.
  # If a batch takes longer than this, it will be cancelled.
  # Default: 45 (longer timeout for complex scanning operations)
  batch_timeout_mins: 45
  # Minimum number of targets in input file to trigger batch processing.
  # Files with fewer targets than this will be processed normally without batching.
  # For example, if set to 1000, files with 2500 targets will create 13 batches of 200 targets each.
  # Default: 1000
  threshold_size: 1000

# Monitor Batch configuration: Settings for processing large monitor target files in batches.
monitor_batch_config:
  # Maximum number of URLs to process in each monitor batch.
  # When input file has more than threshold_size URLs, it will be split into batches of this size.
  # Default: 50 (smaller than scan due to more intensive monitoring operations)
  batch_size: 50
  # Maximum number of concurrent monitor batches to process at the same time.
  # Set to 0 to auto-calculate based on monitor workers (50% of workers, min 1, max 4).
  # Override with specific value if needed.
  # Default: 0 (auto-calculated)
  max_concurrent_batch: 0
  # Timeout for each monitor batch processing in minutes.
  # If a batch takes longer than this, it will be cancelled.
  # Default: 20 (shorter timeout for monitoring operations)
  batch_timeout_mins: 20
  # Minimum number of URLs in input file to trigger batch processing.
  # Files with fewer URLs than this will be processed normally without batching.
  # For example, if set to 200, files with 500 URLs will create 10 batches of 50 URLs each.
  # Default: 200
  threshold_size: 200

# Extractor configuration: Settings for custom path extraction.
extractor_config:
  # List of custom regex patterns to extract paths/URLs from content.
  # These regexes will be applied to the full content of downloaded files (e.g., JS, HTML).
  # Each string in this list should be a valid Go regex.
  # Example using block style (recommended for readability if manually editing):
  custom_regexes:
    - "/(?:[a-zA-Z0-9_\\-]+/)*[a-zA-Z0-9_\\-]+\\.js(?:\\?[^\\s\"]*)?" # Example: Finds /path/to/file.js?query
    - "'(?:[a-zA-Z0-9_\\-]+/)*[a-zA-Z0-9_\\-]+\\.html(?:\\?[^\\s\']*)?'" # Example: Finds '/path/to/file.html'
    - "https?:\\/\\/[^\\s\\\"\\'\\`]+" # Example: Finds http(s)://anything_not_whitespace_or_quote
    # - "another_custom_regex_pattern"
  # If you prefer flow style (ensure correct escaping and quoting if regexes contain special characters):
  # custom_regexes: ["regex1", "regex2"]