# MonsterInc Example Configuration File
# This file shows all available configuration options with their default values and explanations.
# You can copy this file to config.yaml (or config.json) and modify it to your needs.

# Global application mode. Can be "onetime" (run once) or "automated" (run periodically/continuously based on other settings).
# Default: "onetime"
mode: "onetime"
# Path Extractor Domains: List of domains for which path extraction will be attempted
# from JavaScript files. e.g., ["example.com", "api.example.org"]
# Default: []
path_extractor_domains: 

# Input configuration: Specifies where to get target URLs/domains.
input_config:
  # A list of URLs to process directly.
  # Default: []
  input_urls:
    # - http://example.com
    # - https://another.example.org
  # Path to a file containing a list of URLs/domains (one per line).
  # If provided, this file will be read.
  # Default: ""
  input_file: "" # e.g., "/path/to/targets.txt"

# HTTPX Runner configuration: Settings for the httpx tool.
httpx_runner_config:
  # HTTP method to use for requests.
  # Default: "GET"
  method: "GET"
  # Specific URIs to request on each host (appended to the host). Useful for specific file checks.
  # Default: []
  request_uris:
    # - /robots.txt
    # - /.git/config
  # Number of concurrent threads for httpx.
  # Default: 25
  threads: 25
  # Maximum requests per second (0 for no limit).
  # Default: 0
  rate_limit: 0
  # Timeout in seconds for each request.
  # Default: 10
  timeout_secs: 50
  # Number of retries for failed requests.
  # Default: 1
  retries: 2
  # HTTP/SOCKS proxy URL (e.g., http://127.0.0.1:8080, socks5://127.0.0.1:1080).
  # Default: ""
  proxy: ""
  # Whether to follow HTTP redirects.
  # Default: true
  follow_redirects: true
  # Maximum number of redirects to follow.
  # Default: 10
  max_redirects: 10
  # Custom HTTP headers to include in requests (key: value).
  # Default: {}
  custom_headers:
    # User-Agent: "MyCustomScanner/1.0"
    # X-Custom-Header: "SomeValue"
  # Enable verbose output for httpx (more detailed logging).
  # Default: false
  verbose: false
  # Enable technology detection (Wappalyzer).
  # Default: true
  tech_detect: true
  # Extract page titles.
  # Default: true
  extract_title: true
  # Extract HTTP status codes.
  # Default: true
  extract_status_code: true
  # Extract final location after redirects.
  # Default: true
  extract_location: true
  # Extract content length.
  # Default: true
  extract_content_length: true
  # Extract server header.
  # Default: true
  extract_server_header: true
  # Extract content type.
  # Default: true
  extract_content_type: true
  # Extract IP addresses of hosts.
  # Default: true
  extract_ips: true
  # Extract ASN (Autonomous System Number) information.
  # Default: true
  extract_asn: true
  # Extract HTTP response body (can be large, use with caution).
  # Default: false
  extract_body: false
  # Extract HTTP response headers.
  # Default: true
  extract_headers: true
  # Custom DNS resolvers to use (IP addresses).
  # Default: []
  resolvers:
    # - 1.1.1.1
    # - 8.8.8.8
  # Custom ports to scan (comma-separated or list).
  # Overrides default HTTP/HTTPS ports if specified.
  # Default: [] (meaning default ports 80, 443, etc. are used by httpx logic)
  ports:
    # - 80
    # - 443
    # - 8080
    # - "8000-8010"
  # Additional raw httpx flags to pass to the httpx command.
  # Use with caution, as these are passed directly.
  # Default: []
  httpx_flags:
    # - "-silent"
    # - "-no-color"
  # Skip default HTTP/HTTPS ports (80, 443).
  # Default: false
  skip_default_ports: false
  # Deny connections to internal IP address ranges.
  # Default: false
  deny_internal_ips: false

# Crawler configuration: Settings for web crawling.
crawler_config:
  # Initial seed URLs to start crawling from. If empty, httpx results might be used as seeds.
  # Default: []
  seed_urls:
    # - https://example.com/startpage
  # User-Agent string for the crawler.
  # Default: "MonsterIncCrawler/1.0"
  user_agent: "MonsterIncCrawler/1.0"
  # Timeout in seconds for each crawler request.
  # Default: 20
  request_timeout_secs: 30
  # Maximum number of concurrent crawler requests.
  # Default: 10
  max_concurrent_requests: 20
  # Maximum depth to crawl (0 for unlimited, 1 for seed pages only).
  # Default: 5
  max_depth: 5
  # Whether to respect robots.txt directives.
  # Default: true
  respect_robots_txt: false
  # Whether to include subdomains of seed URLs in the crawl.
  # Default: false
  include_subdomains: false
  # List of regex patterns for allowed hostnames. If empty, all are allowed (subject to other scope rules).
  # Default: []
  allowed_host_regex:
    # - .*.example.com
  # List of regex patterns for excluded hostnames.
  # Default: []
  excluded_host_regex:
    # - ads.example.com
  # Crawler scope configuration: Fine-grained control over what the crawler visits.
  scope:
    # List of hostnames the crawler is explicitly disallowed from visiting.
    # Default: []
    disallowed_hostnames:
      # - "internal.example.com"
    # List of subdomains the crawler is explicitly disallowed from visiting.
    # Default: []
    disallowed_subdomains:
      # - "admin" # disallows admin.example.com
    # List of file extensions (with dots) that the crawler should not visit.
    # This uses fast string comparison instead of regex for better performance.
    # Default: [".js", ".txt", ".css", ".xml"]
    disallowed_file_extensions:
      - ".js"
      - ".txt"
      - ".css" 
      - ".xml"
      # - ".pdf"
      # - ".zip"
      # - ".json"
  # Maximum content length in MB to download for a page.
  # Default: 2
  max_content_length_mb: 2
  # Automatically add seed URL hostnames to allowed hostnames list.
  # When enabled, hostnames extracted from seed URLs will be automatically
  # allowed (with highest priority after disallowed rules).
  # This ensures the crawler can always visit seed domains.
  # Default: true
  auto_add_seed_hostnames: true

# Reporter configuration: Settings for generating HTML reports.
reporter_config:
  # Directory where HTML reports will be saved.
  # Default: "reports"
  output_dir: "reports/scan"
  # Number of items to display per page in paginated sections of the report.
  # Default: 25
  items_per_page: 25
  # Whether to embed assets (CSS, JS) directly into the HTML report for a single, portable file.
  # Default: true
  embed_assets: true
  # Path to a custom HTML template for the report. If empty, a default template is used.
  # Default: ""
  template_path: "" # e.g., "/path/to/custom_report_template.html"
  # Whether to generate a report even if no findings/results are available.
  # Default: false
  generate_empty_report: false
  # Title for the generated HTML report.
  # Default: "MonsterInc Scan Report"
  report_title: "MonsterInc Scan Report"
  # Enable DataTables for sortable/searchable tables in the report.
  # Default: true
  enable_data_tables: true
  # MaxProbeResultsPerReportFile defines the maximum number of probe results to include in a single HTML report file.
  # If the total number of probe results exceeds this value, the report will be split into multiple files.
  # A value of 0 means no limit (all results in one file). Default: 1000
  max_probe_results_per_report_file: 1000
  # Note: The exact report filename will be determined by the application logic (e.g., incorporating timestamps or target names).

# Storage configuration: Settings for how scan data is stored (e.g., Parquet files).
storage_config:
  # Base directory path where Parquet files (or other data) will be stored.
  # Default: "data"
  parquet_base_path: "database"
  # Compression codec to use for Parquet files (e.g., "snappy", "gzip", "zstd", "none").
  # Default: "zstd"
  compression_codec: "zstd"

# Notification configuration: Settings for sending notifications (e.g., via Discord).
notification_config:
  # Discord webhook URL for the Scan Service (onetime & automated scans).
  # Default: ""
  scan_service_discord_webhook_url: "" # e.g., "https://discord.com/api/webhooks/your/scan_service_webhook"

  # Discord webhook URL for the File Monitoring Service.
  # Default: ""
  monitor_service_discord_webhook_url: "" # e.g., "https://discord.com/api/webhooks/your/monitor_service_webhook"

  # List of Discord Role IDs to mention in notifications.
  # Default: []
  mention_role_ids:
    # - "123456789012345678" # Example Role ID
  # Send a notification when a scan completes successfully.
  # Default: false
  notify_on_success: true
  # Send a notification when a scan fails.
  # Default: true
  notify_on_failure: true
  # Send a notification when a scan starts.
  # Default: false
  notify_on_scan_start: true
  # Send a notification on critical errors during the application run.
  # Default: true
  notify_on_critical_error: true

  # Automatically delete single diff report files after a Discord notification has been successfully sent.
  # This only deletes individual diff reports (diff_*.html), keeping aggregated reports intact.
  # Default: true
  auto_delete_partial_diff_reports: true

# Logging configuration: Settings for application logging.
log_config:
  # Logging level. Determines the minimum severity of messages to be logged.
  # Valid levels: "debug", "info", "warn", "error", "fatal", "panic".
  # "debug": Detailed information, typically of interest only when diagnosing problems.
  # "info": Confirmation that things are working as expected.
  # "warn": An indication that something unexpected happened, or indicative of some problem in the near future (e.g., "disk space low"). The software is still working as expected.
  # "error": Due to a more serious problem, the software has not been able to perform some function.
  # "fatal": A severe error that will prevent the application from continuing. After logging, the application will exit.
  # "panic": Similar to fatal, but also initiates a panic.
  # Default: "info"
  log_level: "debug"

  # Log output format.
  # Valid formats: "console", "json", "text".
  # "console": Human-readable, colored output for interactive terminals.
  # "json":    Machine-readable JSON objects, one per line.
  # "text":    Plain text output, similar to console but without color codes. Suitable for simple file logging or environments where color is not supported.
  # Default: "console"
  log_format: "console"

  # Path to a log file. If empty, logs are written to standard error (stderr).
  # Example: "/var/log/monsterinc.log" or "monsterinc.log" (for current directory).
  # Default: ""
  log_file: "monsterinc.log"

  # Log rotation settings (applicable when log_file is specified).
  # These settings are handled by an internal log rotation library (lumberjack).

  # Maximum size in megabytes of a log file before it gets rotated.
  # Default: 100
  max_log_size_mb: 100

  # Maximum number of old log files to retain.
  # Default: 3
  max_log_backups: 3

  # Whether to compress old, rotated log files (e.g., with gzip).
  # Default: false
  compress_old_logs: false

# DiffConfig: Configuration for comparing current scan results with previous ones.
diff_config:
  # Number of days to look back for previous scan data to compare against.
  # Default: 7
  previous_scan_lookback_days: 7

# DiffReporterConfig: Configuration for generating content diff reports.
diff_reporter_config:
  # Maximum file size in MB for generating a detailed line-by-line diff.
  # Files larger than this will still be reported as changed, but without a detailed diff.
  # Default: 5
  max_diff_file_size_mb: 5


# MonitorConfig: Configuration for monitoring JS/HTML files for changes.
monitor_config:
  # Whether the monitoring feature is enabled.
  # Default: false
  enabled: true
  # Interval in seconds for how often to check monitored files.
  # Default: 3600 (1 hour)
  check_interval_seconds: 30
  # List of regex or glob patterns for URLs that identify JavaScript files to monitor.
  # Default: [] (Example: ["(.*\\.js)", "(.*\\.mjs)"])
  target_js_file_patterns: []
  # List of regex or glob patterns for URLs that identify HTML files to monitor.
  # Default: [] (Example: ["(.*\\.html?)"])
  target_html_file_patterns: []
  # Maximum number of concurrent checks for monitored files.
  # Default: 5
  max_concurrent_checks: 5
  # Whether to store the full content of a file when a change is detected.
  # If false, only hash/metadata might be stored.
  # Default: false
  store_full_content_on_change: true
  # Timeout in seconds for HTTP requests made by the monitor.
  # Default: 30
  http_timeout_seconds: 600
  # Initial list of specific URLs to monitor. Full URLs are expected.
  # Default: []
  initial_monitor_urls:
    # - https://example.com/main.js
    # - https://example.com/app.js
    # - https://anotherexample.com/index.html
  # Legacy: Comma-separated list of JavaScript file extensions to monitor.
  # Consider using target_js_file_patterns for more flexibility.
  # Default: ["\\.js", "\\.jsx", "\\.ts", "\\.tsx"]
  js_file_extensions:
    - "\\.js"
    - "\\.jsx"
    - "\\.ts"
    - "\\.tsx"
  # Legacy: Comma-separated list of HTML file extensions to monitor.
  # Consider using target_html_file_patterns for more flexibility.
  # Default: ["\\.html", "\\.htm"]
  html_file_extensions:
    - "\\.html"
    - "\\.htm"

  # Aggregation settings for monitor notifications (file changes and errors)
  # How often to send aggregated notifications, in seconds.
  # Default: 600 (10 minutes)
  aggregation_interval_seconds: 10
  # Maximum number of events (changes or errors) to aggregate before sending a notification, regardless of the interval.
  # Default: 10
  max_aggregated_events: 5


# Scheduler configuration: Settings for automated periodic scans (when mode is "automated").
scheduler_config:
  # Scan cycle interval in minutes. The time between the end of one scan and the start of the next.
  # Default: 10080 (equivalent to 7 days)
  cycle_minutes: 3
  # Number of retry attempts if a scheduled scan fails.
  # Default: 2
  retry_attempts: 2
  # Path to the SQLite database file used for storing scan history.
  # Default: "database/scheduler/scheduler_history.db"
  sqlite_db_path: "database/scheduler/scheduler_history.db"

# Extractor configuration: Settings for custom path extraction.
extractor_config:
  # List of custom regex patterns to extract paths/URLs from content.
  # These regexes will be applied to the full content of downloaded files (e.g., JS, HTML).
  # Each string in this list should be a valid Go regex.
  # Example using block style (recommended for readability if manually editing):
  custom_regexes:
    - "/(?:[a-zA-Z0-9_\\-]+/)*[a-zA-Z0-9_\\-]+\\.js(?:\\?[^\\s\"]*)?" # Example: Finds /path/to/file.js?query
    - "'(?:[a-zA-Z0-9_\\-]+/)*[a-zA-Z0-9_\\-]+\\.html(?:\\?[^\\s\']*)?'" # Example: Finds '/path/to/file.html'
    - "https?:\\/\\/[^\\s\\\"\\'\\`]+" # Example: Finds http(s)://anything_not_whitespace_or_quote
    # - "another_custom_regex_pattern"
  # If you prefer flow style (ensure correct escaping and quoting if regexes contain special characters):
  # custom_regexes: ["regex1", "regex2"]

