# MonsterInc Example Configuration File
# This file shows all available configuration options with their default values and explanations.
# You can copy this file to config.yaml (or config.json) and modify it to your needs.

# Global application mode. Can be "onetime" (run once) or "automated" (run periodically/continuously based on other settings).
# Default: "onetime"
mode: "onetime"

# Input configuration: Specifies where to get target URLs/domains.
input_config:
  # A list of URLs to process directly.
  # Default: []
  input_urls:
    # - http://example.com
    # - https://another.example.org
  # Path to a file containing a list of URLs/domains (one per line).
  # If provided, this file will be read.
  # Default: ""
  input_file: "" # e.g., "/path/to/targets.txt"

# HTTPX Runner configuration: Settings for the httpx tool.
httpx_runner_config:
  # HTTP method to use for requests.
  # Default: "GET"
  method: "GET"
  # Specific URIs to request on each host (appended to the host). Useful for specific file checks.
  # Default: []
  request_uris:
    # - /robots.txt
    # - /.git/config
  # Number of concurrent threads for httpx.
  # Default: 25
  threads: 25
  # Maximum requests per second (0 for no limit).
  # Default: 0
  rate_limit: 0
  # Timeout in seconds for each request.
  # Default: 10
  timeout_secs: 50
  # Number of retries for failed requests.
  # Default: 1
  retries: 2
  # HTTP/SOCKS proxy URL (e.g., http://127.0.0.1:8080, socks5://127.0.0.1:1080).
  # Default: ""
  proxy: ""
  # Whether to follow HTTP redirects.
  # Default: true
  follow_redirects: true
  # Maximum number of redirects to follow.
  # Default: 10
  max_redirects: 10
  # Custom HTTP headers to include in requests (key: value).
  # Default: {}
  custom_headers:
    # User-Agent: "MyCustomScanner/1.0"
    # X-Custom-Header: "SomeValue"
  # Enable verbose output for httpx (more detailed logging).
  # Default: false
  verbose: false
  # Enable technology detection (Wappalyzer).
  # Default: true
  tech_detect: true
  # Extract page titles.
  # Default: true
  extract_title: true
  # Extract HTTP status codes.
  # Default: true
  extract_status_code: true
  # Extract final location after redirects.
  # Default: true
  extract_location: true
  # Extract content length.
  # Default: true
  extract_content_length: true
  # Extract server header.
  # Default: true
  extract_server_header: true
  # Extract content type.
  # Default: true
  extract_content_type: true
  # Extract IP addresses of hosts.
  # Default: true
  extract_ips: true
  # Extract HTTP response body (can be large, use with caution).
  # Default: false
  extract_body: false
  # Extract HTTP response headers.
  # Default: true
  extract_headers: true
  # Custom DNS resolvers to use (IP addresses).
  # Default: []
  resolvers:
    # - 1.1.1.1
    # - 8.8.8.8
  # Custom ports to scan (comma-separated or list).
  # Overrides default HTTP/HTTPS ports if specified.
  # Default: [] (meaning default ports 80, 443, etc. are used by httpx logic)
  ports:
    # - 80
    # - 443
    # - 8080
    # - "8000-8010"
  # Additional raw httpx flags to pass to the httpx command.
  # Use with caution, as these are passed directly.
  # Default: []
  httpx_flags:
    # - "-silent"
    # - "-no-color"
  # Skip default HTTP/HTTPS ports (80, 443).
  # Default: false
  skip_default_ports: false
  # Deny connections to internal IP address ranges.
  # Default: false
  deny_internal_ips: false

# Crawler configuration: Settings for web crawling.
crawler_config:
  # Initial seed URLs to start crawling from. If empty, httpx results might be used as seeds.
  # Default: []
  seed_urls:
    # - https://example.com/startpage
  # User-Agent string for the crawler.
  # Default: "MonsterIncCrawler/1.0"
  user_agent: "MonsterIncCrawler/1.0"
  # Timeout in seconds for each crawler request.
  # Default: 20
  request_timeout_secs: 30
  # Maximum number of concurrent crawler requests.
  # Default: 10
  max_concurrent_requests: 20
  # Maximum depth to crawl (0 for unlimited, 1 for seed pages only).
  # Default: 5
  max_depth: 5
  # Whether to respect robots.txt directives.
  # Default: true
  respect_robots_txt: false
  # Whether to include subdomains of seed URLs in the crawl.
  # Default: false
  include_subdomains: false
  # List of regex patterns for allowed hostnames. If empty, all are allowed (subject to other scope rules).
  # Default: []
  allowed_host_regex:
    # - .*.example.com
  # List of regex patterns for excluded hostnames.
  # Default: []
  excluded_host_regex:
    # - ads.example.com
  # Crawler scope configuration: Fine-grained control over what the crawler visits.
  scope:
    # List of hostnames the crawler is allowed to visit.
    # Default: [] (meaning all hostnames are allowed if not disallowed)
    allowed_hostnames:
      # - "www.example.com"
      # - "api.example.com"
    # List of subdomains the crawler is allowed to visit (relative to seed domains or allowed_hostnames).
    # Default: []
    allowed_subdomains:
      # - "blog" # allows blog.example.com if example.com is a seed
    # List of hostnames the crawler is explicitly disallowed from visiting.
    # Default: []
    disallowed_hostnames:
      # - "internal.example.com"
    # List of subdomains the crawler is explicitly disallowed from visiting.
    # Default: []
    disallowed_subdomains:
      # - "admin" # disallows admin.example.com
    # List of regex patterns for URL paths the crawler is allowed to visit.
    # Default: [] (meaning all paths are allowed if not disallowed)
    allowed_path_regexes:
      # - "/products/.*"
    # List of regex patterns for URL paths the crawler is explicitly disallowed from visiting.
    # Default: []
    disallowed_path_regexes:
      # - "/admin/.*"
      - "\.js$"
      - "\.txt$"
  # Maximum content length in MB to download for a page.
  # Default: 2
  max_content_length_mb: 2

# Reporter configuration: Settings for generating HTML reports.
reporter_config:
  # Directory where HTML reports will be saved.
  # Default: "reports"
  output_dir: "reports"
  # Number of items to display per page in paginated sections of the report.
  # Default: 25
  items_per_page: 25
  # Whether to embed assets (CSS, JS) directly into the HTML report for a single, portable file.
  # Default: true
  embed_assets: true
  # Path to a custom HTML template for the report. If empty, a default template is used.
  # Default: ""
  template_path: "" # e.g., "/path/to/custom_report_template.html"
  # Whether to generate a report even if no findings/results are available.
  # Default: false
  generate_empty_report: false
  # Title for the generated HTML report.
  # Default: "MonsterInc Scan Report"
  report_title: "MonsterInc Scan Report"
  # Enable DataTables for sortable/searchable tables in the report.
  # Default: true
  enable_data_tables: true
  # Note: The exact report filename will be determined by the application logic (e.g., incorporating timestamps or target names).

# Storage configuration: Settings for how scan data is stored (e.g., Parquet files).
storage_config:
  # Base directory path where Parquet files (or other data) will be stored.
  # Default: "data"
  parquet_base_path: "database"
  # Compression codec to use for Parquet files (e.g., "snappy", "gzip", "zstd", "none").
  # Default: "zstd"
  compression_codec: "zstd"

# Notification configuration: Settings for sending notifications (e.g., via Discord).
notification_config:
  # Discord webhook URL to send notifications to.
  # Default: ""
  discord_webhook_url: "" # e.g., "https://discord.com/api/webhooks/your/webhook"
  # List of Discord Role IDs to mention in notifications.
  # Default: []
  mention_role_ids:
    # - "123456789012345678" # Example Role ID
  # Send a notification when a scan completes successfully.
  # Default: false
  notify_on_success: false
  # Send a notification when a scan fails.
  # Default: true
  notify_on_failure: true
  # Send a notification when a scan starts.
  # Default: false
  notify_on_scan_start: false
  # Send a notification on critical errors during the application run.
  # Default: true
  notify_on_critical_error: true

# Logging configuration: Settings for application logging.
log_config:
  # Logging level (e.g., "debug", "info", "warn", "error", "fatal", "panic").
  # Default: "info"
  log_level: "info"
  # Log output format ("console", "text", "json").
  # "console" is human-readable, colored output. "text" is plain.
  # Default: "console"
  log_format: "console"
  # Path to a log file. If empty, logs are written to stderr.
  # Default: ""
  log_file: "" # e.g., "/var/log/monsterinc.log"
  # Maximum size in megabytes of a log file before it gets rotated.
  # Default: 100
  max_log_size_mb: 100
  # Maximum number of old log files to retain.
  # Default: 3
  max_log_backups: 3
  # Whether to compress old, rotated log files (e.g., with gzip).
  # Default: false
  compress_old_logs: false

# DiffConfig: Configuration for comparing current scan results with previous ones.
diff_config:
  # Number of days to look back for previous scan data to compare against.
  # Default: 7
  previous_scan_lookback_days: 7

# MonitorConfig: Configuration for monitoring JS/HTML files for changes (e.g., in a continuous mode).
monitor_config:
  # Comma-separated list of JavaScript file extensions to monitor.
  # Default: [".js", ".jsx", ".ts", ".tsx"] (derived from ".js,.jsx,.ts,.tsx")
  js_file_extensions:
    - ".js"
    - ".jsx"
    - ".ts"
    - ".tsx"
  # Comma-separated list of HTML file extensions to monitor.
  # Default: [".html", ".htm"] (derived from ".html,.htm")
  html_file_extensions:
    - ".html"
    - ".htm"

# NormalizerConfig: Configuration related to URL normalization (currently minimal).
# This section might be expanded in the future if more complex normalization rules are needed.
normalizer_config: {}
  # Default scheme to assume if a URL is provided without one (e.g., "example.com" -> "http://example.com").
  # Default: "http" (This was an example, ensure it aligns with actual NormalizerConfig struct if present)
  # default_scheme: "http"

# Scheduler configuration: Settings for automated periodic scans (when mode is "automated").
scheduler_config:
  # Scan cycle interval in minutes. The time between the end of one scan and the start of the next.
  # Default: 10080 (equivalent to 7 days)
  cycle_minutes: 10080
  # Number of retry attempts if a scheduled scan fails.
  # Default: 2
  retry_attempts: 2
  # Path to the SQLite database file used for storing scan history.
  # Default: "database/scheduler_history.db"
  sqlite_db_path: "database/scheduler_history.db"